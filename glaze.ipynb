{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glaze paper notes\n",
    "[Glaze](https://arxiv.org/pdf/2302.04222.pdf) \n",
    "\n",
    "- perceptual pertubation budget: `p`\n",
    "- impact of input perturbation: `alpha`\n",
    "- Given artwork `x`\n",
    "- feature exterator `phi`\n",
    "- style-transferred version of x into target style `T:omega(x,T)`\n",
    "- Style cloak `delta_x`\n",
    "- Stable Diffusion provides both `epsilon` and `theta`\n",
    "\n",
    "![Alt text](figures/glaze_eq1.png)\n",
    "\n",
    "1) choose target style that is difrent than og artist style\n",
    "2) transfer og artists images to new style using style transfer\n",
    "3) compute cloak pertubation using [LPIPS](https://github.com/richzhang/PerceptualSimilarity)\n",
    "\n",
    "![Alt text](figures/glaze_eq2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Implementing glaze paper\n",
    "[HuggingFace stable diffusion pipleine](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py)\n",
    "\n",
    "### Feature extractor\n",
    "[CLIP](https://huggingface.co/docs/transformers/model_doc/clip)\n",
    "\n",
    "[image_processing_clip](https://github.com/huggingface/transformers/blob/v4.26.1/src/transformers/models/clip/image_processing_clip.py#L45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%pip install Pillow\n",
    "%pip install transformers\n",
    "%pip install torch torchvision\n",
    "%pip install clip\n",
    "%pip install matplotlib\n",
    "%pip install lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from clip import clip\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# Load CLIP feature extractor and model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device=device)\n",
    "model.eval()\n",
    "\n",
    "# Load images A and B + perturbation\n",
    "img_a = preprocess(Image.open('dog.png')).unsqueeze(0).to(device)\n",
    "img_b = preprocess(Image.open('cow.png')).unsqueeze(0).to(device)\n",
    "perturbation = torch.randn_like(img_b, requires_grad=True)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam([perturbation], lr=0.01)\n",
    "\n",
    "# Loop to adjust perturbation and minimize loss\n",
    "for i in range(1000):\n",
    "    # Compute features for img_a and img_b + perturbation\n",
    "    features_a = model.encode_image(img_a)\n",
    "    features_b = model.encode_image(img_b + perturbation)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(features_a, features_b)\n",
    "    # loss = torch.norm(features_a - features_b, p=2)\n",
    "\n",
    "    # Zero gradients, compute gradients, and update perturbation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Step {i}: Loss = {loss.item()}')\n",
    "\n",
    "# Apply perturbation to img_b and save perturbed image\n",
    "perturbed_img_b = (img_b + perturbation).squeeze(0).cpu()\n",
    "#transforms.ToPILImage()(perturbed_img_b).save('path/to/perturbed_image_b.jpg')\n",
    "# Convert the tensor image to a NumPy array\n",
    "img_array = perturbed_img_b.detach().numpy().transpose(1, 2, 0)\n",
    "\n",
    "# Display the image using matplotlib\n",
    "plt.imshow(img_array)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import lpips\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from clip import clip\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load CLIP feature extractor and model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device=device)\n",
    "model.eval()\n",
    "\n",
    "# Load images A and B + perturbation\n",
    "img_a = preprocess(Image.open('dog.png')).unsqueeze(0).to(device)\n",
    "img_b = preprocess(Image.open('cow.png')).unsqueeze(0).to(device)\n",
    "perturbation = torch.randn_like(img_b, requires_grad=True)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam([perturbation], lr=0.01)\n",
    "\n",
    "# Define penalty weight and lpips loss function\n",
    "penalty_weight = 0.1\n",
    "p = 0.05\n",
    "lpips_loss = lpips.LPIPS(net='vgg').to(device)\n",
    "\n",
    "# Loop to adjust perturbation and minimize loss\n",
    "for i in range(1000):\n",
    "    # Compute features for img_a and img_b + perturbation\n",
    "    features_a = model.encode_image(img_a)\n",
    "    features_b = model.encode_image(img_b + perturbation)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = torch.norm(features_a - features_b, p=2) + penalty_weight * lpips_loss(perturbation - p, torch.zeros_like(perturbation))\n",
    "\n",
    "    # Zero gradients, compute gradients, and update perturbation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Step {i}: Loss = {loss.item()}')\n",
    "\n",
    "# Apply perturbation to img_b and save perturbed image\n",
    "perturbed_img_b = (img_b + perturbation).squeeze(0).cpu()\n",
    "vutils.save_image(perturbed_img_b, 'cow_dog.png')\n",
    "# transforms.ToPILImage()(perturbed_img_b).save('perturbed_image_b.png')\n",
    "# Convert the tensor image to a NumPy array\n",
    "img_array = perturbed_img_b.detach().numpy().transpose(1, 2, 0)\n",
    "\n",
    "# Display the image using matplotlib\n",
    "plt.imshow(img_array)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 9.7",
   "language": "sage",
   "name": "SageMath-9.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "sage",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
